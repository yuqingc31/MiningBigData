{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Frequent Itemset Mining and Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frequent Itemset Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association mining, also known as association rule mining, uncovers relationships between items in large datasets. It identifies patterns, dependencies, and correlations, commonly used in market basket analysis, web usage mining, and more. Key concepts include itemsets, support, confidence, and association rules. The process involves data collection, preprocessing, generating itemsets, calculating support, rule generation, and rule evaluation. Applications range from optimizing product placement in retail to healthcare analytics and fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Retail Market Basket Analysis\n",
    "\n",
    "\n",
    "### Objective:\n",
    "A retail store aims to improve its sales strategy by understanding customer purchasing patterns and optimizing product placement. The goal is to identify associations between products and generate actionable insights for cross-selling and promotion strategies.\n",
    "\n",
    "### Data:\n",
    "The dataset consists of transaction records, each representing items purchased by a customer during a single visit.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "### Data Collection: \n",
    "Gather transaction data, recording items purchased by customers.\n",
    "\n",
    "### Data Preprocessing:\n",
    "Clean the data, handle missing values, and organize it into transactional format.\n",
    "\n",
    "### Generate Itemsets:\n",
    "Identify frequent itemsets – combinations of products frequently bought together.\n",
    "\n",
    "### Calculate Support:\n",
    "Measure support for each itemset, indicating how often they occur in transactions.\n",
    "\n",
    "### Generate Association Rules:\n",
    "Create rules based on user-defined thresholds for support and confidence.\n",
    "\n",
    "### Evaluate Rules:\n",
    "Analyze the generated rules using metrics like confidence and lift.\n",
    "\n",
    "#### Example Rules:\n",
    "\n",
    "**Rule 1:** {Bread} ➔ {Butter} (Support: 10%, Confidence: 60%)\n",
    "If a customer buys bread, there is a 60% chance they will also buy butter.\n",
    "\n",
    "**Rule 2:** {Milk, Eggs} ➔ {Bread} (Support: 8%, Confidence: 70%)\n",
    "If a customer buys milk and eggs, there is a 70% chance they will also buy bread.\n",
    "\n",
    "\n",
    "### Insights:\n",
    "Customers frequently buy Bread and Butter together, suggesting a bundling opportunity.\n",
    "Milk and Eggs purchasers are likely to buy Bread, indicating potential cross-selling.\n",
    "Use these insights to optimize product placement, create targeted promotions, and improve the overall customer experience.\n",
    "\n",
    "\n",
    "### Benefits:\n",
    "Increased sales through strategic product bundling.\n",
    "Improved customer satisfaction by offering relevant product recommendations.\n",
    "Enhanced inventory management through better understanding of product associations.\n",
    "This case study demonstrates how association mining can provide actionable insights for retailers to optimize their sales strategy, enhance customer experience, and boost overall profitability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 1.4/1.4 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sspec (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sspec (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "   support         itemsets\n",
      "0      0.6           (Beer)\n",
      "1      0.8          (Bread)\n",
      "2      0.6        (Diapers)\n",
      "3      0.8           (Milk)\n",
      "4      0.6  (Diapers, Beer)\n",
      "5      0.6    (Bread, Milk)\n",
      "\n",
      "Association Rules:\n",
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0   (Diapers)      (Beer)                 0.6                 0.6      0.6   \n",
      "1      (Beer)   (Diapers)                 0.6                 0.6      0.6   \n",
      "2     (Bread)      (Milk)                 0.8                 0.8      0.6   \n",
      "3      (Milk)     (Bread)                 0.8                 0.8      0.6   \n",
      "\n",
      "   confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0        1.00  1.666667      0.24         inf           1.00  \n",
      "1        1.00  1.666667      0.24         inf           1.00  \n",
      "2        0.75  0.937500     -0.04         0.8          -0.25  \n",
      "3        0.75  0.937500     -0.04         0.8          -0.25  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.frequent_patterns import fpmax, fpgrowth\n",
    "import pandas as pd\n",
    "\n",
    "# Sample transaction data\n",
    "transactions = [\n",
    "    ['Bread', 'Milk', 'Eggs'],\n",
    "    ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
    "    ['Milk', 'Diapers', 'Beer', 'Cola'],\n",
    "    ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
    "    ['Bread', 'Milk', 'Cola']\n",
    "]\n",
    "\n",
    "# Convert the transaction data to a one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Generate frequent itemsets using Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Display the frequent itemsets and association rules\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "**transactions** represent individual shopping baskets.\n",
    "The data is converted to a **one-hot encoded** format using the TransactionEncoder.\n",
    "The **Apriori algorithm** is used to find frequent itemsets.\n",
    "\n",
    "Association rules are generated based on a confidence threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1: Apply the code to a new dataset\n",
    "\n",
    "Please reuse the above code as needed to process the CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction list size is:  7501\n"
     ]
    }
   ],
   "source": [
    "#Download data and convert to list\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv', header=None)\n",
    "\n",
    "#Fill null:\n",
    "data.fillna(0,inplace=True)\n",
    "\n",
    "# Convert data to list format for each transaction, (hint: use a list called transactions):\n",
    "\n",
    "transactions = []\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    transactions.append([str(data.values[i,j]) for j in range(0,20) if str(data.values[i,j])!='0'])\n",
    "#     print(i, end=' ')\n",
    "print('Transaction list size is: ',len(transactions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "    support         itemsets\n",
      "0  0.163845      (chocolate)\n",
      "1  0.179709           (eggs)\n",
      "2  0.170911   (french fries)\n",
      "3  0.132116      (green tea)\n",
      "4  0.129583           (milk)\n",
      "5  0.238368  (mineral water)\n",
      "6  0.174110      (spaghetti)\n",
      "\n",
      "Association Rules:\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, leverage, conviction, zhangs_metric]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Convert the transaction data to a one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Generate frequent itemsets using Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "if len(frequent_itemsets) == 0:\n",
    "    print(\"frequent_itemsets is empty!\")\n",
    "else:\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "    # Display the frequent itemsets and association rules\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    print(frequent_itemsets)\n",
    "\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    print(rules)\n",
    "## Note: for metric definitions reref to: https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2: Using FP-Growth for Association Pattern Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mlxtend library to get association rules for the same dataset and parameters as in Activity 1.\n",
    "\n",
    "Compare results. Then change min support and test Apriori vs FP-Growth. Plot the timing results for both algorithms and discuss differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mineral water)</td>\n",
       "      <td>(green tea)</td>\n",
       "      <td>0.238368</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>0.130313</td>\n",
       "      <td>0.986357</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>0.997927</td>\n",
       "      <td>-0.017837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(green tea)</td>\n",
       "      <td>(mineral water)</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.238368</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>0.235116</td>\n",
       "      <td>0.986357</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>0.995748</td>\n",
       "      <td>-0.015688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(spaghetti)</td>\n",
       "      <td>(green tea)</td>\n",
       "      <td>0.174110</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.152374</td>\n",
       "      <td>1.153335</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>1.023900</td>\n",
       "      <td>0.160977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(green tea)</td>\n",
       "      <td>(spaghetti)</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.174110</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.200807</td>\n",
       "      <td>1.153335</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>1.033405</td>\n",
       "      <td>0.153188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(french fries)</td>\n",
       "      <td>(green tea)</td>\n",
       "      <td>0.170911</td>\n",
       "      <td>0.132116</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.166927</td>\n",
       "      <td>1.263488</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>1.041786</td>\n",
       "      <td>0.251529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>(frozen vegetables)</td>\n",
       "      <td>(cake)</td>\n",
       "      <td>0.095321</td>\n",
       "      <td>0.081056</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>1.328618</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>1.029851</td>\n",
       "      <td>0.273399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>(cake)</td>\n",
       "      <td>(frozen vegetables)</td>\n",
       "      <td>0.081056</td>\n",
       "      <td>0.095321</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.126645</td>\n",
       "      <td>1.328618</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>1.035866</td>\n",
       "      <td>0.269155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>(pancakes)</td>\n",
       "      <td>(cake)</td>\n",
       "      <td>0.095054</td>\n",
       "      <td>0.081056</td>\n",
       "      <td>0.011865</td>\n",
       "      <td>0.124825</td>\n",
       "      <td>1.539983</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>1.050011</td>\n",
       "      <td>0.387473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>(cake)</td>\n",
       "      <td>(pancakes)</td>\n",
       "      <td>0.081056</td>\n",
       "      <td>0.095054</td>\n",
       "      <td>0.011865</td>\n",
       "      <td>0.146382</td>\n",
       "      <td>1.539983</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>1.060129</td>\n",
       "      <td>0.381571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>(cereals)</td>\n",
       "      <td>(mineral water)</td>\n",
       "      <td>0.025730</td>\n",
       "      <td>0.238368</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.398964</td>\n",
       "      <td>1.673729</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.267198</td>\n",
       "      <td>0.413162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             antecedents          consequents  antecedent support  \\\n",
       "0        (mineral water)          (green tea)            0.238368   \n",
       "1            (green tea)      (mineral water)            0.132116   \n",
       "2            (spaghetti)          (green tea)            0.174110   \n",
       "3            (green tea)          (spaghetti)            0.132116   \n",
       "4         (french fries)          (green tea)            0.170911   \n",
       "..                   ...                  ...                 ...   \n",
       "315  (frozen vegetables)               (cake)            0.095321   \n",
       "316               (cake)  (frozen vegetables)            0.081056   \n",
       "317           (pancakes)               (cake)            0.095054   \n",
       "318               (cake)           (pancakes)            0.081056   \n",
       "319            (cereals)      (mineral water)            0.025730   \n",
       "\n",
       "     consequent support   support  confidence      lift  leverage  conviction  \\\n",
       "0              0.132116  0.031063    0.130313  0.986357 -0.000430    0.997927   \n",
       "1              0.238368  0.031063    0.235116  0.986357 -0.000430    0.995748   \n",
       "2              0.132116  0.026530    0.152374  1.153335  0.003527    1.023900   \n",
       "3              0.174110  0.026530    0.200807  1.153335  0.003527    1.033405   \n",
       "4              0.132116  0.028530    0.166927  1.263488  0.005950    1.041786   \n",
       "..                  ...       ...         ...       ...       ...         ...   \n",
       "315            0.081056  0.010265    0.107692  1.328618  0.002539    1.029851   \n",
       "316            0.095321  0.010265    0.126645  1.328618  0.002539    1.035866   \n",
       "317            0.081056  0.011865    0.124825  1.539983  0.004160    1.050011   \n",
       "318            0.095054  0.011865    0.146382  1.539983  0.004160    1.060129   \n",
       "319            0.238368  0.010265    0.398964  1.673729  0.004132    1.267198   \n",
       "\n",
       "     zhangs_metric  \n",
       "0        -0.017837  \n",
       "1        -0.015688  \n",
       "2         0.160977  \n",
       "3         0.153188  \n",
       "4         0.251529  \n",
       "..             ...  \n",
       "315       0.273399  \n",
       "316       0.269155  \n",
       "317       0.387473  \n",
       "318       0.381571  \n",
       "319       0.413162  \n",
       "\n",
       "[320 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets = fpgrowth(df, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation systems, often referred to as recommender systems, are crucial components in today's digital landscape, powering personalized content delivery in diverse platforms such as e-commerce, streaming services, and social media. The primary goal of recommendation systems is to predict and suggest items or content that users are likely to be interested in based on their historical preferences, behaviors, and interactions. These systems employ various algorithms, including collaborative filtering, content-based filtering, and hybrid approaches, to analyze user data and generate accurate and relevant recommendations. By enhancing user experience through tailored suggestions, recommendation systems contribute significantly to user engagement, customer satisfaction, and business success in the competitive online environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Movie Recommendation System\n",
    "\n",
    "### Objective:\n",
    "Netflix aims to improve user satisfaction and retention by implementing an advanced movie \n",
    "recommendation system. The goal is to provide personalized movie suggestions to users based on their viewing history and preferences.\n",
    "\n",
    "### Data:\n",
    "The dataset includes user interactions with the platform, such as watched movies, ratings given, and genres liked.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "### Data Collection:\n",
    "Gather user interaction data, including watched movies, ratings, and genre preferences.\n",
    "\n",
    "### Data Preprocessing:\n",
    "Clean the data, handle missing values, and organize it into a format suitable for recommendation algorithms.\n",
    "\n",
    "### User Profiling:\n",
    "Create user profiles based on historical data, considering factors like preferred genres, average ratings, and watch history.\n",
    "\n",
    "### Recommendation Algorithm:\n",
    "Implement collaborative filtering and content-based filtering algorithms to generate personalized movie recommendations.\n",
    "\n",
    "### Evaluation:\n",
    "Assess the system's performance using metrics such as precision, recall, and mean absolute error to ensure accurate and relevant recommendations.\n",
    "\n",
    "### Example Scenario:\n",
    "A user who frequently watches science fiction movies and has given high ratings to several sci-fi films might receive recommendations for newly released sci-fi titles.\n",
    "\n",
    "### Benefits:\n",
    "Improved user engagement and satisfaction through personalized recommendations.\n",
    "Increased user retention as users discover content aligned with their preferences.\n",
    "Enhanced platform competitiveness in the streaming industry.\n",
    "\n",
    "This case study illustrates how a movie recommendation system, by leveraging user data and advanced algorithms, can significantly enhance the streaming experience, leading to increased user satisfaction and platform success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System implementation\n",
    "\n",
    "The skeleton code below is created so that you can have a go at writing your own implementation of collaborative filtering.\n",
    "\n",
    "Collaborative filtering is a popular technique used in recommendation systems to personalise and improve user experiences. The concept behind collaborative filtering is to analyse the behaviour and preferences of a group of users to recommend items or content to another user based on their similarities with the group. This technique can be applied to various types of data, such as movies, music, books, and products. Collaborative filtering works by building a model that identifies patterns and similarities in user behaviour and then uses these patterns to predict what items a user is likely to enjoy. By leveraging the collective intelligence of a group, collaborative filtering algorithms can generate highly accurate recommendations, making it a powerful tool for e-commerce, content-based websites, and other recommendation-based systems. In this way, collaborative filtering enables businesses to offer personalised experiences to their users, which can lead to increased engagement, loyalty, and revenue.\n",
    "\n",
    "The pseudocode is explained as:\n",
    "\n",
    "1. Collect data on user preferences for a set of items.\n",
    "2. Represent the user preferences as a matrix, with each row representing a user and each column representing an item.\n",
    "3. Compute the similarity between each pair of users using a similarity metric, such as cosine similarity or Pearson correlation.\n",
    "4. For a target user, identify the top N most similar users based on the similarity metric.\n",
    "5. For each item the target user has not rated, predict the rating by computing the weighted average of the ratings given by the most similar users, where the weights are the similarities between the users and the target user.\n",
    "6. Recommend the top N items with the highest predicted ratings.\n",
    "\n",
    "### Activity 3: Item based Collaborative Filtering Implementation\n",
    "\n",
    "Complete the code below to implement CF recommender. Debug the code and make it working using given small rating table.\n",
    "\n",
    "When the code is working properly, use the provided Netflix movie rating files to obtain recommendations for a target user (just a user number). \n",
    "\n",
    "\"movies.csv\" file contains movie titles. Therefore (optionally) you can replace movie ids with titles from that file.\n",
    "\n",
    "HAVE A GO! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def similarity(user1, user2):\n",
    "    # Calculate the dot product of the two user vectors\n",
    "    dot_product = np.dot(user1, user2)\n",
    "    # Calculate the magnitude of the two user vectors\n",
    "    magnitude = np.sqrt(np.sum(user1 ** 2) * np.sum(user2 ** 2))\n",
    "    # Calculate the similarity between the two users\n",
    "    similarity = dot_product/magnitude\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_ratings, movie_ratings):\n",
    "    # Find the indices of the users who rated the movie\n",
    "    rated_indices = np.where(movie_ratings != 0)[0]\n",
    "    # Get the ratings of the movie by the rated users\n",
    "    ratings = movie_ratings[rated_indices]\n",
    "    # Get the user vectors of the rated users\n",
    "    rated_users = user_ratings[rated_indices]\n",
    "    # Calculate the similarities between the rated users and the target user\n",
    "    similarities = [similarity(user_ratings[0], rated_users[i]) for i in range(len(rated_indices))]\n",
    "    # Calculate the weighted average of the ratings, using the similarities as weights\n",
    "    weighted_sum = np.dot(similarities, ratings)\n",
    "    weighted_sum /= np.sum(similarities)\n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies: ['Movie 2', 'Movie 1']\n"
     ]
    }
   ],
   "source": [
    "def recommend_movies(user_ratings, target_user):\n",
    "    # Get the number of users and movies\n",
    "    num_users, num_movies = user_ratings.shape\n",
    "    # Find the indices of the unwatched movies by the target user\n",
    "    unwatched_indices = np.where(user_ratings[target_user] == 0)[0]\n",
    "    # Predict the ratings for the unwatched movies\n",
    "    predicted_ratings = [predict_rating(user_ratings, user_ratings[:, movie_index]) for movie_index in unwatched_indices]\n",
    "    # Sort the movies by the predicted rating in descending order\n",
    "    sorted_indices = np.argsort(predicted_ratings)[::-1]\n",
    "    # Get the top 3 recommended movies\n",
    "    top_movies = sorted_indices[:3]\n",
    "    recommended_movies = [f\"Movie {i+1}\" for i in top_movies]\n",
    "    return recommended_movies\n",
    "\n",
    "# Create a sample ratings matrix\n",
    "ratings = np.array([[3, 0, 0, 5], [0, 4, 0, 3], [1, 0, 2, 4], [5, 0, 3, 0], [0, 2, 4, 0]])\n",
    "\n",
    "# Make movie recommendations for the target user\n",
    "# Choose a target user to make recommendations for\n",
    "target_user = 3\n",
    "recommended_movies = recommend_movies(ratings, target_user)\n",
    "\n",
    "# Print the recommended movies\n",
    "print(\"Recommended movies:\", recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
      "movieId                                                    ...                  \n",
      "1        4.0  0.0  0.0  0.0  4.0  0.0  4.5  0.0  0.0  0.0  ...  4.0  0.0  4.0   \n",
      "2        0.0  0.0  0.0  0.0  0.0  4.0  0.0  4.0  0.0  0.0  ...  0.0  4.0  0.0   \n",
      "3        4.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4        0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "5        0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "193581   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "193583   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "193585   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "193587   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "193609   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "userId   604  605  606  607  608  609  610  \n",
      "movieId                                     \n",
      "1        3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
      "2        5.0  3.5  0.0  0.0  2.0  0.0  0.0  \n",
      "3        0.0  0.0  0.0  0.0  2.0  0.0  0.0  \n",
      "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5        3.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  \n",
      "193581   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "193583   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "193585   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "193587   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "193609   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[9724 rows x 610 columns]>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/So-ham/Movie-Recommendation-System/blob/main/movie-recommendation.ipynb \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "final_dataset = ratings.pivot(index='movieId',columns='userId',values='rating')\n",
    "final_dataset.fillna(0,inplace=True)\n",
    "print(final_dataset.head)\n",
    "\n",
    "no_user_voted = ratings.groupby('movieId')['rating'].agg('count')\n",
    "no_movies_voted = ratings.groupby('userId')['rating'].agg('count')\n",
    "\n",
    "final_dataset = final_dataset.loc[no_user_voted[no_user_voted > 10].index,:]\n",
    "final_dataset = final_dataset.loc[:,no_movies_voted[no_movies_voted > 50].index]\n",
    "ratings = final_dataset.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies: ['Movie 199', 'Movie 290', 'Movie 101']\n"
     ]
    }
   ],
   "source": [
    "target_user = 200\n",
    "recommended_movies = recommend_movies(ratings, target_user)\n",
    "\n",
    "# Print the recommended movies\n",
    "print(\"Recommended movies:\", recommended_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4: Tiktok Case Study\n",
    "\n",
    "Read the following article, discussing with your classmates, and respond to the following questions:\n",
    "\n",
    "Article: https://www.popsci.com/technology/tiktok-algorithm/ Why TikTok's algorithm is so addictive? \n",
    "\n",
    "The idea behind these questions is to prompt you to think more holistically about the industry on which you are studying. \n",
    "\n",
    "Technical skills can be taught, but it is crucial to consider the impact of the work you are undertaking, including the threats compared to the benefits.\n",
    "\n",
    "Questions: \n",
    "\n",
    "1) How does TikTok's recommendation algorithm leverage user interactions, such as likes, comments, watch time, and shares, to personalize the content feed?\n",
    "\n",
    "2) Do you think the level of personalization described in the article enhances or limits the user experience on TikTok?\n",
    "\n",
    "3) How do TikTok's human content moderators complement the work of the algorithm?\n",
    "\n",
    "4) What challenges and benefits might arise from the collaboration between automated algorithms and human moderation in content platforms?\n",
    "\n",
    "5) The article suggests that rapid growth can pose challenges for platforms like TikTok. In what ways might fast-paced growth impact a platform's ability to address and mitigate potential harms?\n",
    "\n",
    "6) The article mentions concerns about TikTok users encountering harmful content as their streams become more niche. What measures could platforms take to address this issue and protect users, especially considering TikTok's younger user base?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
