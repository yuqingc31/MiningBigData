{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Frequent Itemset Mining and Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frequent Itemset Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association mining, also known as association rule mining, uncovers relationships between items in large datasets. It identifies patterns, dependencies, and correlations, commonly used in market basket analysis, web usage mining, and more. Key concepts include itemsets, support, confidence, and association rules. The process involves data collection, preprocessing, generating itemsets, calculating support, rule generation, and rule evaluation. Applications range from optimizing product placement in retail to healthcare analytics and fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Retail Market Basket Analysis\n",
    "\n",
    "\n",
    "### Objective:\n",
    "A retail store aims to improve its sales strategy by understanding customer purchasing patterns and optimizing product placement. The goal is to identify associations between products and generate actionable insights for cross-selling and promotion strategies.\n",
    "\n",
    "### Data:\n",
    "The dataset consists of transaction records, each representing items purchased by a customer during a single visit.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "### Data Collection: \n",
    "Gather transaction data, recording items purchased by customers.\n",
    "\n",
    "### Data Preprocessing:\n",
    "Clean the data, handle missing values, and organize it into transactional format.\n",
    "\n",
    "### Generate Itemsets:\n",
    "Identify frequent itemsets – combinations of products frequently bought together.\n",
    "\n",
    "### Calculate Support:\n",
    "Measure support for each itemset, indicating how often they occur in transactions.\n",
    "\n",
    "### Generate Association Rules:\n",
    "Create rules based on user-defined thresholds for support and confidence.\n",
    "\n",
    "### Evaluate Rules:\n",
    "Analyze the generated rules using metrics like confidence and lift.\n",
    "\n",
    "#### Example Rules:\n",
    "\n",
    "**Rule 1:** {Bread} ➔ {Butter} (Support: 10%, Confidence: 60%)\n",
    "If a customer buys bread, there is a 60% chance they will also buy butter.\n",
    "\n",
    "**Rule 2:** {Milk, Eggs} ➔ {Bread} (Support: 8%, Confidence: 70%)\n",
    "If a customer buys milk and eggs, there is a 70% chance they will also buy bread.\n",
    "\n",
    "\n",
    "### Insights:\n",
    "Customers frequently buy Bread and Butter together, suggesting a bundling opportunity.\n",
    "Milk and Eggs purchasers are likely to buy Bread, indicating potential cross-selling.\n",
    "Use these insights to optimize product placement, create targeted promotions, and improve the overall customer experience.\n",
    "\n",
    "\n",
    "### Benefits:\n",
    "Increased sales through strategic product bundling.\n",
    "Improved customer satisfaction by offering relevant product recommendations.\n",
    "Enhanced inventory management through better understanding of product associations.\n",
    "This case study demonstrates how association mining can provide actionable insights for retailers to optimize their sales strategy, enhance customer experience, and boost overall profitability.\n",
    "\n",
    "\n",
    "Review the below code to understand how it works. You wil use this code later in Activity 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install mlxend if needed\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "   support         itemsets\n",
      "0      0.6           (Beer)\n",
      "1      0.8          (Bread)\n",
      "2      0.6        (Diapers)\n",
      "3      0.8           (Milk)\n",
      "4      0.6  (Beer, Diapers)\n",
      "5      0.6    (Milk, Bread)\n",
      "\n",
      "Association Rules:\n",
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0      (Beer)   (Diapers)                 0.6                 0.6      0.6   \n",
      "1   (Diapers)      (Beer)                 0.6                 0.6      0.6   \n",
      "2      (Milk)     (Bread)                 0.8                 0.8      0.6   \n",
      "3     (Bread)      (Milk)                 0.8                 0.8      0.6   \n",
      "\n",
      "   confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0        1.00  1.666667      0.24         inf           1.00  \n",
      "1        1.00  1.666667      0.24         inf           1.00  \n",
      "2        0.75  0.937500     -0.04         0.8          -0.25  \n",
      "3        0.75  0.937500     -0.04         0.8          -0.25  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.frequent_patterns import fpmax, fpgrowth\n",
    "import pandas as pd\n",
    "\n",
    "# Sample transaction data\n",
    "transactions = [\n",
    "    ['Bread', 'Milk', 'Eggs'],\n",
    "    ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
    "    ['Milk', 'Diapers', 'Beer', 'Cola'],\n",
    "    ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
    "    ['Bread', 'Milk', 'Cola']\n",
    "]\n",
    "\n",
    "# Convert the transaction data to a one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Generate frequent itemsets using Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Display the frequent itemsets and association rules\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "**transactions** represent individual shopping baskets.\n",
    "The data is converted to a **one-hot encoded** format using the TransactionEncoder.\n",
    "The **Apriori algorithm** is used to find frequent itemsets.\n",
    "\n",
    "Association rules are generated based on a confidence threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1: Apply the code to a new dataset\n",
    "\n",
    "Please reuse the above code as needed to process the CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "     support                       itemsets\n",
      "0   0.999867                            (0)\n",
      "1   0.087188                      (burgers)\n",
      "2   0.081056                         (cake)\n",
      "3   0.059992                      (chicken)\n",
      "4   0.163845                    (chocolate)\n",
      "5   0.080389                      (cookies)\n",
      "6   0.051060                  (cooking oil)\n",
      "7   0.179709                         (eggs)\n",
      "8   0.079323                     (escalope)\n",
      "9   0.170911                 (french fries)\n",
      "10  0.063325              (frozen smoothie)\n",
      "11  0.095321            (frozen vegetables)\n",
      "12  0.052393                (grated cheese)\n",
      "13  0.132116                    (green tea)\n",
      "14  0.098254                  (ground beef)\n",
      "15  0.076523               (low fat yogurt)\n",
      "16  0.129583                         (milk)\n",
      "17  0.238368                (mineral water)\n",
      "18  0.065858                    (olive oil)\n",
      "19  0.095054                     (pancakes)\n",
      "20  0.071457                       (shrimp)\n",
      "21  0.050527                         (soup)\n",
      "22  0.174110                    (spaghetti)\n",
      "23  0.068391                     (tomatoes)\n",
      "24  0.062525                       (turkey)\n",
      "25  0.058526             (whole wheat rice)\n",
      "26  0.087188                   (burgers, 0)\n",
      "27  0.081056                      (cake, 0)\n",
      "28  0.059992                   (0, chicken)\n",
      "29  0.163845                 (chocolate, 0)\n",
      "30  0.080389                   (cookies, 0)\n",
      "31  0.051060               (0, cooking oil)\n",
      "32  0.179709                      (0, eggs)\n",
      "33  0.079323                  (escalope, 0)\n",
      "34  0.170911              (french fries, 0)\n",
      "35  0.063192           (frozen smoothie, 0)\n",
      "36  0.095321         (frozen vegetables, 0)\n",
      "37  0.052393             (0, grated cheese)\n",
      "38  0.131982                 (green tea, 0)\n",
      "39  0.098254               (0, ground beef)\n",
      "40  0.076390            (low fat yogurt, 0)\n",
      "41  0.129583                      (0, milk)\n",
      "42  0.238235             (mineral water, 0)\n",
      "43  0.065725                 (olive oil, 0)\n",
      "44  0.095054                  (pancakes, 0)\n",
      "45  0.071324                    (0, shrimp)\n",
      "46  0.050527                      (soup, 0)\n",
      "47  0.174110                 (0, spaghetti)\n",
      "48  0.068391                  (tomatoes, 0)\n",
      "49  0.062525                    (turkey, 0)\n",
      "50  0.058526          (whole wheat rice, 0)\n",
      "51  0.052660     (mineral water, chocolate)\n",
      "52  0.050927          (mineral water, eggs)\n",
      "53  0.059725     (mineral water, spaghetti)\n",
      "54  0.052660  (chocolate, mineral water, 0)\n",
      "55  0.050927       (mineral water, 0, eggs)\n",
      "56  0.059725  (mineral water, 0, spaghetti)\n",
      "\n",
      "Association Rules:\n",
      "                   antecedents consequents  antecedent support  \\\n",
      "0                    (burgers)         (0)            0.087188   \n",
      "1                       (cake)         (0)            0.081056   \n",
      "2                    (chicken)         (0)            0.059992   \n",
      "3                  (chocolate)         (0)            0.163845   \n",
      "4                    (cookies)         (0)            0.080389   \n",
      "5                (cooking oil)         (0)            0.051060   \n",
      "6                       (eggs)         (0)            0.179709   \n",
      "7                   (escalope)         (0)            0.079323   \n",
      "8               (french fries)         (0)            0.170911   \n",
      "9            (frozen smoothie)         (0)            0.063325   \n",
      "10         (frozen vegetables)         (0)            0.095321   \n",
      "11             (grated cheese)         (0)            0.052393   \n",
      "12                 (green tea)         (0)            0.132116   \n",
      "13               (ground beef)         (0)            0.098254   \n",
      "14            (low fat yogurt)         (0)            0.076523   \n",
      "15                      (milk)         (0)            0.129583   \n",
      "16             (mineral water)         (0)            0.238368   \n",
      "17                 (olive oil)         (0)            0.065858   \n",
      "18                  (pancakes)         (0)            0.095054   \n",
      "19                    (shrimp)         (0)            0.071457   \n",
      "20                      (soup)         (0)            0.050527   \n",
      "21                 (spaghetti)         (0)            0.174110   \n",
      "22                  (tomatoes)         (0)            0.068391   \n",
      "23                    (turkey)         (0)            0.062525   \n",
      "24          (whole wheat rice)         (0)            0.058526   \n",
      "25  (mineral water, chocolate)         (0)            0.052660   \n",
      "26       (mineral water, eggs)         (0)            0.050927   \n",
      "27  (mineral water, spaghetti)         (0)            0.059725   \n",
      "\n",
      "    consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0             0.999867  0.087188    1.000000  1.000133  0.000012         inf   \n",
      "1             0.999867  0.081056    1.000000  1.000133  0.000011         inf   \n",
      "2             0.999867  0.059992    1.000000  1.000133  0.000008         inf   \n",
      "3             0.999867  0.163845    1.000000  1.000133  0.000022         inf   \n",
      "4             0.999867  0.080389    1.000000  1.000133  0.000011         inf   \n",
      "5             0.999867  0.051060    1.000000  1.000133  0.000007         inf   \n",
      "6             0.999867  0.179709    1.000000  1.000133  0.000024         inf   \n",
      "7             0.999867  0.079323    1.000000  1.000133  0.000011         inf   \n",
      "8             0.999867  0.170911    1.000000  1.000133  0.000023         inf   \n",
      "9             0.999867  0.063192    0.997895  0.998028 -0.000125    0.063325   \n",
      "10            0.999867  0.095321    1.000000  1.000133  0.000013         inf   \n",
      "11            0.999867  0.052393    1.000000  1.000133  0.000007         inf   \n",
      "12            0.999867  0.131982    0.998991  0.999124 -0.000116    0.132116   \n",
      "13            0.999867  0.098254    1.000000  1.000133  0.000013         inf   \n",
      "14            0.999867  0.076390    0.998258  0.998391 -0.000123    0.076523   \n",
      "15            0.999867  0.129583    1.000000  1.000133  0.000017         inf   \n",
      "16            0.999867  0.238235    0.999441  0.999574 -0.000102    0.238368   \n",
      "17            0.999867  0.065725    0.997976  0.998109 -0.000125    0.065858   \n",
      "18            0.999867  0.095054    1.000000  1.000133  0.000013         inf   \n",
      "19            0.999867  0.071324    0.998134  0.998267 -0.000124    0.071457   \n",
      "20            0.999867  0.050527    1.000000  1.000133  0.000007         inf   \n",
      "21            0.999867  0.174110    1.000000  1.000133  0.000023         inf   \n",
      "22            0.999867  0.068391    1.000000  1.000133  0.000009         inf   \n",
      "23            0.999867  0.062525    1.000000  1.000133  0.000008         inf   \n",
      "24            0.999867  0.058526    1.000000  1.000133  0.000008         inf   \n",
      "25            0.999867  0.052660    1.000000  1.000133  0.000007         inf   \n",
      "26            0.999867  0.050927    1.000000  1.000133  0.000007         inf   \n",
      "27            0.999867  0.059725    1.000000  1.000133  0.000008         inf   \n",
      "\n",
      "    zhangs_metric  \n",
      "0        0.000146  \n",
      "1        0.000145  \n",
      "2        0.000142  \n",
      "3        0.000159  \n",
      "4        0.000145  \n",
      "5        0.000140  \n",
      "6        0.000163  \n",
      "7        0.000145  \n",
      "8        0.000161  \n",
      "9       -0.002105  \n",
      "10       0.000147  \n",
      "11       0.000141  \n",
      "12      -0.001009  \n",
      "13       0.000148  \n",
      "14      -0.001742  \n",
      "15       0.000153  \n",
      "16      -0.000559  \n",
      "17      -0.002024  \n",
      "18       0.000147  \n",
      "19      -0.001866  \n",
      "20       0.000140  \n",
      "21       0.000161  \n",
      "22       0.000143  \n",
      "23       0.000142  \n",
      "24       0.000142  \n",
      "25       0.000141  \n",
      "26       0.000140  \n",
      "27       0.000142  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 读取数据并指定数据类型为字符串\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv', header=None, dtype=str)\n",
    "\n",
    "# 填充缺失值\n",
    "data.fillna('0', inplace=True)\n",
    "\n",
    "# 转换为列表格式\n",
    "transactions = data.values.tolist()\n",
    "\n",
    "# 转换为独热编码格式\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# 生成频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
    "\n",
    "if len(frequent_itemsets) == 0:\n",
    "    print(\"frequent_itemsets is empty!\")\n",
    "else:\n",
    "    # 生成关联规则\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "    # 显示频繁项集和关联规则\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    print(frequent_itemsets)\n",
    "\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data and convert to list\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv', header=None, dtype=str)\n",
    "\n",
    "#Fill null:\n",
    "data.fillna('0',inplace=True)\n",
    "\n",
    "''' \n",
    "    Convert data to list of lists format for each transaction, (hint: use a list called transactions):\n",
    "    each sub-list should contain items from one line of the file, e.g. \n",
    "    [[turkey,burgers,mineral,water,eggs,cooking oil],[...],...]\n",
    "'''\n",
    "\n",
    "# Store the list of lists in transactions\n",
    "\n",
    "transactions = data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "     support                       itemsets\n",
      "0   0.999867                            (0)\n",
      "1   0.087188                      (burgers)\n",
      "2   0.081056                         (cake)\n",
      "3   0.059992                      (chicken)\n",
      "4   0.163845                    (chocolate)\n",
      "5   0.080389                      (cookies)\n",
      "6   0.051060                  (cooking oil)\n",
      "7   0.179709                         (eggs)\n",
      "8   0.079323                     (escalope)\n",
      "9   0.170911                 (french fries)\n",
      "10  0.063325              (frozen smoothie)\n",
      "11  0.095321            (frozen vegetables)\n",
      "12  0.052393                (grated cheese)\n",
      "13  0.132116                    (green tea)\n",
      "14  0.098254                  (ground beef)\n",
      "15  0.076523               (low fat yogurt)\n",
      "16  0.129583                         (milk)\n",
      "17  0.238368                (mineral water)\n",
      "18  0.065858                    (olive oil)\n",
      "19  0.095054                     (pancakes)\n",
      "20  0.071457                       (shrimp)\n",
      "21  0.050527                         (soup)\n",
      "22  0.174110                    (spaghetti)\n",
      "23  0.068391                     (tomatoes)\n",
      "24  0.062525                       (turkey)\n",
      "25  0.058526             (whole wheat rice)\n",
      "26  0.087188                   (burgers, 0)\n",
      "27  0.081056                      (cake, 0)\n",
      "28  0.059992                   (0, chicken)\n",
      "29  0.163845                 (chocolate, 0)\n",
      "30  0.080389                   (cookies, 0)\n",
      "31  0.051060               (0, cooking oil)\n",
      "32  0.179709                      (0, eggs)\n",
      "33  0.079323                  (escalope, 0)\n",
      "34  0.170911              (french fries, 0)\n",
      "35  0.063192           (frozen smoothie, 0)\n",
      "36  0.095321         (frozen vegetables, 0)\n",
      "37  0.052393             (0, grated cheese)\n",
      "38  0.131982                 (green tea, 0)\n",
      "39  0.098254               (0, ground beef)\n",
      "40  0.076390            (low fat yogurt, 0)\n",
      "41  0.129583                      (0, milk)\n",
      "42  0.238235             (mineral water, 0)\n",
      "43  0.065725                 (olive oil, 0)\n",
      "44  0.095054                  (pancakes, 0)\n",
      "45  0.071324                    (0, shrimp)\n",
      "46  0.050527                      (soup, 0)\n",
      "47  0.174110                 (0, spaghetti)\n",
      "48  0.068391                  (tomatoes, 0)\n",
      "49  0.062525                    (turkey, 0)\n",
      "50  0.058526          (whole wheat rice, 0)\n",
      "51  0.052660     (mineral water, chocolate)\n",
      "52  0.050927          (mineral water, eggs)\n",
      "53  0.059725     (mineral water, spaghetti)\n",
      "54  0.052660  (chocolate, mineral water, 0)\n",
      "55  0.050927       (mineral water, 0, eggs)\n",
      "56  0.059725  (mineral water, 0, spaghetti)\n",
      "\n",
      "Association Rules:\n",
      "                   antecedents consequents  antecedent support  \\\n",
      "0                    (burgers)         (0)            0.087188   \n",
      "1                       (cake)         (0)            0.081056   \n",
      "2                    (chicken)         (0)            0.059992   \n",
      "3                  (chocolate)         (0)            0.163845   \n",
      "4                    (cookies)         (0)            0.080389   \n",
      "5                (cooking oil)         (0)            0.051060   \n",
      "6                       (eggs)         (0)            0.179709   \n",
      "7                   (escalope)         (0)            0.079323   \n",
      "8               (french fries)         (0)            0.170911   \n",
      "9            (frozen smoothie)         (0)            0.063325   \n",
      "10         (frozen vegetables)         (0)            0.095321   \n",
      "11             (grated cheese)         (0)            0.052393   \n",
      "12                 (green tea)         (0)            0.132116   \n",
      "13               (ground beef)         (0)            0.098254   \n",
      "14            (low fat yogurt)         (0)            0.076523   \n",
      "15                      (milk)         (0)            0.129583   \n",
      "16             (mineral water)         (0)            0.238368   \n",
      "17                 (olive oil)         (0)            0.065858   \n",
      "18                  (pancakes)         (0)            0.095054   \n",
      "19                    (shrimp)         (0)            0.071457   \n",
      "20                      (soup)         (0)            0.050527   \n",
      "21                 (spaghetti)         (0)            0.174110   \n",
      "22                  (tomatoes)         (0)            0.068391   \n",
      "23                    (turkey)         (0)            0.062525   \n",
      "24          (whole wheat rice)         (0)            0.058526   \n",
      "25  (mineral water, chocolate)         (0)            0.052660   \n",
      "26       (mineral water, eggs)         (0)            0.050927   \n",
      "27  (mineral water, spaghetti)         (0)            0.059725   \n",
      "\n",
      "    consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0             0.999867  0.087188    1.000000  1.000133  0.000012         inf   \n",
      "1             0.999867  0.081056    1.000000  1.000133  0.000011         inf   \n",
      "2             0.999867  0.059992    1.000000  1.000133  0.000008         inf   \n",
      "3             0.999867  0.163845    1.000000  1.000133  0.000022         inf   \n",
      "4             0.999867  0.080389    1.000000  1.000133  0.000011         inf   \n",
      "5             0.999867  0.051060    1.000000  1.000133  0.000007         inf   \n",
      "6             0.999867  0.179709    1.000000  1.000133  0.000024         inf   \n",
      "7             0.999867  0.079323    1.000000  1.000133  0.000011         inf   \n",
      "8             0.999867  0.170911    1.000000  1.000133  0.000023         inf   \n",
      "9             0.999867  0.063192    0.997895  0.998028 -0.000125    0.063325   \n",
      "10            0.999867  0.095321    1.000000  1.000133  0.000013         inf   \n",
      "11            0.999867  0.052393    1.000000  1.000133  0.000007         inf   \n",
      "12            0.999867  0.131982    0.998991  0.999124 -0.000116    0.132116   \n",
      "13            0.999867  0.098254    1.000000  1.000133  0.000013         inf   \n",
      "14            0.999867  0.076390    0.998258  0.998391 -0.000123    0.076523   \n",
      "15            0.999867  0.129583    1.000000  1.000133  0.000017         inf   \n",
      "16            0.999867  0.238235    0.999441  0.999574 -0.000102    0.238368   \n",
      "17            0.999867  0.065725    0.997976  0.998109 -0.000125    0.065858   \n",
      "18            0.999867  0.095054    1.000000  1.000133  0.000013         inf   \n",
      "19            0.999867  0.071324    0.998134  0.998267 -0.000124    0.071457   \n",
      "20            0.999867  0.050527    1.000000  1.000133  0.000007         inf   \n",
      "21            0.999867  0.174110    1.000000  1.000133  0.000023         inf   \n",
      "22            0.999867  0.068391    1.000000  1.000133  0.000009         inf   \n",
      "23            0.999867  0.062525    1.000000  1.000133  0.000008         inf   \n",
      "24            0.999867  0.058526    1.000000  1.000133  0.000008         inf   \n",
      "25            0.999867  0.052660    1.000000  1.000133  0.000007         inf   \n",
      "26            0.999867  0.050927    1.000000  1.000133  0.000007         inf   \n",
      "27            0.999867  0.059725    1.000000  1.000133  0.000008         inf   \n",
      "\n",
      "    zhangs_metric  \n",
      "0        0.000146  \n",
      "1        0.000145  \n",
      "2        0.000142  \n",
      "3        0.000159  \n",
      "4        0.000145  \n",
      "5        0.000140  \n",
      "6        0.000163  \n",
      "7        0.000145  \n",
      "8        0.000161  \n",
      "9       -0.002105  \n",
      "10       0.000147  \n",
      "11       0.000141  \n",
      "12      -0.001009  \n",
      "13       0.000148  \n",
      "14      -0.001742  \n",
      "15       0.000153  \n",
      "16      -0.000559  \n",
      "17      -0.002024  \n",
      "18       0.000147  \n",
      "19      -0.001866  \n",
      "20       0.000140  \n",
      "21       0.000161  \n",
      "22       0.000143  \n",
      "23       0.000142  \n",
      "24       0.000142  \n",
      "25       0.000141  \n",
      "26       0.000140  \n",
      "27       0.000142  \n"
     ]
    }
   ],
   "source": [
    "# Convert the transaction data to a one-hot encoded format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Generate frequent itemsets using Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
    "\n",
    "if len(frequent_itemsets) == 0:\n",
    "    print(\"frequent_itemsets is empty!\")\n",
    "else:\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "    # 显示频繁项集和关联规则\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    print(frequent_itemsets)\n",
    "\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    print(rules)\n",
    "\n",
    "## Note: for metric definitions reref to: https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2: Using FP-Growth for Association Pattern Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mlxtend library to get association rules for the same dataset and parameters as in Activity 1.\n",
    "\n",
    "Compare results. Then change min support and test Apriori vs FP-Growth. Plot the timing results for both algorithms and discuss differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support                       itemsets\n",
      "0   0.999867                            (0)\n",
      "1   0.087188                      (burgers)\n",
      "2   0.081056                         (cake)\n",
      "3   0.059992                      (chicken)\n",
      "4   0.163845                    (chocolate)\n",
      "5   0.080389                      (cookies)\n",
      "6   0.051060                  (cooking oil)\n",
      "7   0.179709                         (eggs)\n",
      "8   0.079323                     (escalope)\n",
      "9   0.170911                 (french fries)\n",
      "10  0.063325              (frozen smoothie)\n",
      "11  0.095321            (frozen vegetables)\n",
      "12  0.052393                (grated cheese)\n",
      "13  0.132116                    (green tea)\n",
      "14  0.098254                  (ground beef)\n",
      "15  0.076523               (low fat yogurt)\n",
      "16  0.129583                         (milk)\n",
      "17  0.238368                (mineral water)\n",
      "18  0.065858                    (olive oil)\n",
      "19  0.095054                     (pancakes)\n",
      "20  0.071457                       (shrimp)\n",
      "21  0.050527                         (soup)\n",
      "22  0.174110                    (spaghetti)\n",
      "23  0.068391                     (tomatoes)\n",
      "24  0.062525                       (turkey)\n",
      "25  0.058526             (whole wheat rice)\n",
      "26  0.087188                   (burgers, 0)\n",
      "27  0.081056                      (cake, 0)\n",
      "28  0.059992                   (0, chicken)\n",
      "29  0.163845                 (chocolate, 0)\n",
      "30  0.080389                   (cookies, 0)\n",
      "31  0.051060               (0, cooking oil)\n",
      "32  0.179709                      (0, eggs)\n",
      "33  0.079323                  (escalope, 0)\n",
      "34  0.170911              (french fries, 0)\n",
      "35  0.063192           (frozen smoothie, 0)\n",
      "36  0.095321         (frozen vegetables, 0)\n",
      "37  0.052393             (0, grated cheese)\n",
      "38  0.131982                 (green tea, 0)\n",
      "39  0.098254               (0, ground beef)\n",
      "40  0.076390            (low fat yogurt, 0)\n",
      "41  0.129583                      (0, milk)\n",
      "42  0.238235             (mineral water, 0)\n",
      "43  0.065725                 (olive oil, 0)\n",
      "44  0.095054                  (pancakes, 0)\n",
      "45  0.071324                    (0, shrimp)\n",
      "46  0.050527                      (soup, 0)\n",
      "47  0.174110                 (0, spaghetti)\n",
      "48  0.068391                  (tomatoes, 0)\n",
      "49  0.062525                    (turkey, 0)\n",
      "50  0.058526          (whole wheat rice, 0)\n",
      "51  0.052660     (mineral water, chocolate)\n",
      "52  0.050927          (mineral water, eggs)\n",
      "53  0.059725     (mineral water, spaghetti)\n",
      "54  0.052660  (chocolate, mineral water, 0)\n",
      "55  0.050927       (mineral water, 0, eggs)\n",
      "56  0.059725  (mineral water, 0, spaghetti)\n",
      "                   antecedents consequents  antecedent support  \\\n",
      "0                    (burgers)         (0)            0.087188   \n",
      "1                       (cake)         (0)            0.081056   \n",
      "2                    (chicken)         (0)            0.059992   \n",
      "3                  (chocolate)         (0)            0.163845   \n",
      "4                    (cookies)         (0)            0.080389   \n",
      "5                (cooking oil)         (0)            0.051060   \n",
      "6                       (eggs)         (0)            0.179709   \n",
      "7                   (escalope)         (0)            0.079323   \n",
      "8               (french fries)         (0)            0.170911   \n",
      "9            (frozen smoothie)         (0)            0.063325   \n",
      "10         (frozen vegetables)         (0)            0.095321   \n",
      "11             (grated cheese)         (0)            0.052393   \n",
      "12                 (green tea)         (0)            0.132116   \n",
      "13               (ground beef)         (0)            0.098254   \n",
      "14            (low fat yogurt)         (0)            0.076523   \n",
      "15                      (milk)         (0)            0.129583   \n",
      "16             (mineral water)         (0)            0.238368   \n",
      "17                 (olive oil)         (0)            0.065858   \n",
      "18                  (pancakes)         (0)            0.095054   \n",
      "19                    (shrimp)         (0)            0.071457   \n",
      "20                      (soup)         (0)            0.050527   \n",
      "21                 (spaghetti)         (0)            0.174110   \n",
      "22                  (tomatoes)         (0)            0.068391   \n",
      "23                    (turkey)         (0)            0.062525   \n",
      "24          (whole wheat rice)         (0)            0.058526   \n",
      "25  (mineral water, chocolate)         (0)            0.052660   \n",
      "26       (mineral water, eggs)         (0)            0.050927   \n",
      "27  (mineral water, spaghetti)         (0)            0.059725   \n",
      "\n",
      "    consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0             0.999867  0.087188    1.000000  1.000133  0.000012         inf   \n",
      "1             0.999867  0.081056    1.000000  1.000133  0.000011         inf   \n",
      "2             0.999867  0.059992    1.000000  1.000133  0.000008         inf   \n",
      "3             0.999867  0.163845    1.000000  1.000133  0.000022         inf   \n",
      "4             0.999867  0.080389    1.000000  1.000133  0.000011         inf   \n",
      "5             0.999867  0.051060    1.000000  1.000133  0.000007         inf   \n",
      "6             0.999867  0.179709    1.000000  1.000133  0.000024         inf   \n",
      "7             0.999867  0.079323    1.000000  1.000133  0.000011         inf   \n",
      "8             0.999867  0.170911    1.000000  1.000133  0.000023         inf   \n",
      "9             0.999867  0.063192    0.997895  0.998028 -0.000125    0.063325   \n",
      "10            0.999867  0.095321    1.000000  1.000133  0.000013         inf   \n",
      "11            0.999867  0.052393    1.000000  1.000133  0.000007         inf   \n",
      "12            0.999867  0.131982    0.998991  0.999124 -0.000116    0.132116   \n",
      "13            0.999867  0.098254    1.000000  1.000133  0.000013         inf   \n",
      "14            0.999867  0.076390    0.998258  0.998391 -0.000123    0.076523   \n",
      "15            0.999867  0.129583    1.000000  1.000133  0.000017         inf   \n",
      "16            0.999867  0.238235    0.999441  0.999574 -0.000102    0.238368   \n",
      "17            0.999867  0.065725    0.997976  0.998109 -0.000125    0.065858   \n",
      "18            0.999867  0.095054    1.000000  1.000133  0.000013         inf   \n",
      "19            0.999867  0.071324    0.998134  0.998267 -0.000124    0.071457   \n",
      "20            0.999867  0.050527    1.000000  1.000133  0.000007         inf   \n",
      "21            0.999867  0.174110    1.000000  1.000133  0.000023         inf   \n",
      "22            0.999867  0.068391    1.000000  1.000133  0.000009         inf   \n",
      "23            0.999867  0.062525    1.000000  1.000133  0.000008         inf   \n",
      "24            0.999867  0.058526    1.000000  1.000133  0.000008         inf   \n",
      "25            0.999867  0.052660    1.000000  1.000133  0.000007         inf   \n",
      "26            0.999867  0.050927    1.000000  1.000133  0.000007         inf   \n",
      "27            0.999867  0.059725    1.000000  1.000133  0.000008         inf   \n",
      "\n",
      "    zhangs_metric  \n",
      "0        0.000146  \n",
      "1        0.000145  \n",
      "2        0.000142  \n",
      "3        0.000159  \n",
      "4        0.000145  \n",
      "5        0.000140  \n",
      "6        0.000163  \n",
      "7        0.000145  \n",
      "8        0.000161  \n",
      "9       -0.002105  \n",
      "10       0.000147  \n",
      "11       0.000141  \n",
      "12      -0.001009  \n",
      "13       0.000148  \n",
      "14      -0.001742  \n",
      "15       0.000153  \n",
      "16      -0.000559  \n",
      "17      -0.002024  \n",
      "18       0.000147  \n",
      "19      -0.001866  \n",
      "20       0.000140  \n",
      "21       0.000161  \n",
      "22       0.000143  \n",
      "23       0.000142  \n",
      "24       0.000142  \n",
      "25       0.000141  \n",
      "26       0.000140  \n",
      "27       0.000142  \n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation systems, often referred to as recommender systems, are crucial components in today's digital landscape, powering personalized content delivery in diverse platforms such as e-commerce, streaming services, and social media. The primary goal of recommendation systems is to predict and suggest items or content that users are likely to be interested in based on their historical preferences, behaviors, and interactions. These systems employ various algorithms, including collaborative filtering, content-based filtering, and hybrid approaches, to analyze user data and generate accurate and relevant recommendations. By enhancing user experience through tailored suggestions, recommendation systems contribute significantly to user engagement, customer satisfaction, and business success in the competitive online environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Movie Recommendation System\n",
    "\n",
    "### Objective:\n",
    "Netflix aims to improve user satisfaction and retention by implementing an advanced movie \n",
    "recommendation system. The goal is to provide personalized movie suggestions to users based on their viewing history and preferences.\n",
    "\n",
    "### Data:\n",
    "The dataset includes user interactions with the platform, such as watched movies, ratings given, and genres liked.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "### Data Collection:\n",
    "Gather user interaction data, including watched movies, ratings, and genre preferences.\n",
    "\n",
    "### Data Preprocessing:\n",
    "Clean the data, handle missing values, and organize it into a format suitable for recommendation algorithms.\n",
    "\n",
    "### User Profiling:\n",
    "Create user profiles based on historical data, considering factors like preferred genres, average ratings, and watch history.\n",
    "\n",
    "### Recommendation Algorithm:\n",
    "Implement collaborative filtering and content-based filtering algorithms to generate personalized movie recommendations.\n",
    "\n",
    "### Evaluation:\n",
    "Assess the system's performance using metrics such as precision, recall, and mean absolute error to ensure accurate and relevant recommendations.\n",
    "\n",
    "### Example Scenario:\n",
    "A user who frequently watches science fiction movies and has given high ratings to several sci-fi films might receive recommendations for newly released sci-fi titles.\n",
    "\n",
    "### Benefits:\n",
    "Improved user engagement and satisfaction through personalized recommendations.\n",
    "Increased user retention as users discover content aligned with their preferences.\n",
    "Enhanced platform competitiveness in the streaming industry.\n",
    "\n",
    "This case study illustrates how a movie recommendation system, by leveraging user data and advanced algorithms, can significantly enhance the streaming experience, leading to increased user satisfaction and platform success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System implementation\n",
    "\n",
    "The skeleton code below is created so that you can have a go at writing your own implementation of collaborative filtering.\n",
    "\n",
    "Collaborative filtering is a popular technique used in recommendation systems to personalise and improve user experiences. The concept behind collaborative filtering is to analyse the behaviour and preferences of a group of users to recommend items or content to another user based on their similarities with the group. This technique can be applied to various types of data, such as movies, music, books, and products. Collaborative filtering works by building a model that identifies patterns and similarities in user behaviour and then uses these patterns to predict what items a user is likely to enjoy. By leveraging the collective intelligence of a group, collaborative filtering algorithms can generate highly accurate recommendations, making it a powerful tool for e-commerce, content-based websites, and other recommendation-based systems. In this way, collaborative filtering enables businesses to offer personalised experiences to their users, which can lead to increased engagement, loyalty, and revenue.\n",
    "\n",
    "The pseudocode is explained as:\n",
    "\n",
    "1. Collect data on user preferences for a set of items.\n",
    "2. Represent the user preferences as a matrix, with each row representing a user and each column representing an item.\n",
    "3. Compute the similarity between each pair of users using a similarity metric, such as cosine similarity or Pearson correlation.\n",
    "4. For a target user, identify the top N most similar users based on the similarity metric.\n",
    "5. For each item the target user has not rated, predict the rating by computing the weighted average of the ratings given by the most similar users, where the weights are the similarities between the users and the target user.\n",
    "6. Recommend the top N items with the highest predicted ratings.\n",
    "\n",
    "### Activity 3: Item based Collaborative Filtering Implementation\n",
    "\n",
    "Complete the code below to implement CF recommender. Debug the code and make it working using given small rating table.\n",
    "\n",
    "When the code is working properly, use the provided Netflix movie rating files to obtain recommendations for a target user (just a user number). \n",
    "\n",
    "\"movies.csv\" file contains movie titles. Therefore (optionally) you can replace movie ids with titles from that file.\n",
    "\n",
    "HAVE A GO! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def similarity(user1, user2):\n",
    "    # Calculate the dot product of the two user vectors\n",
    "    dot_product = np.dot(user1, user2)\n",
    "    \n",
    "    # Calculate the magnitude of the two user vectors\n",
    "    magnitude_user1 = np.linalg.norm(user1)\n",
    "    magnitude_user2 = np.linalg.norm(user2)\n",
    "    \n",
    "    # Calculate the similarity between the two users\n",
    "    similarity = dot_product / (magnitude_user1 * magnitude_user2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_ratings, movie_ratings):\n",
    "    # Find the indices of the users who rated the movie\n",
    "    rated_indices = np.where(movie_ratings != 0)[0]\n",
    "    \n",
    "    # Get the ratings of the movie by the rated users\n",
    "    ratings = movie_ratings[rated_indices]\n",
    "    \n",
    "    # Get the user vectors of the rated users\n",
    "    rated_users = user_ratings[rated_indices]\n",
    "    \n",
    "    # Calculate the similarities between the rated users and the target user\n",
    "    similarities = np.array([similarity(user_ratings, rated_user) for rated_user in rated_users])\n",
    "    \n",
    "    # Calculate the weighted sum of the ratings, using the similarities as weights\n",
    "    weighted_sum = np.sum(ratings * similarities)\n",
    "    \n",
    "    return weighted_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yuqing/Desktop/MiningBigData/workshop mbd/workshop5/Workshop week 10 Association Mining and Recommender Systems.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Make movie recommendations for the target user\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Choose a target user to make recommendations for\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m target_user \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m recommended_movies \u001b[39m=\u001b[39m recommend_movies(ratings, target_user)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Print the recommended movies\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecommended movies:\u001b[39m\u001b[39m\"\u001b[39m, recommended_movies)\n",
      "\u001b[1;32m/Users/yuqing/Desktop/MiningBigData/workshop mbd/workshop5/Workshop week 10 Association Mining and Recommender Systems.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m unwatched_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(user_ratings[target_user] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Predict the ratings for the unwatched movies\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predicted_ratings \u001b[39m=\u001b[39m [predict_rating(user_ratings[target_user], user_ratings[:, movie_index]) \u001b[39mfor\u001b[39;00m movie_index \u001b[39min\u001b[39;00m unwatched_indices]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Sort the movies by the predicted rating in descending order\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m sorted_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(predicted_ratings)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/Users/yuqing/Desktop/MiningBigData/workshop mbd/workshop5/Workshop week 10 Association Mining and Recommender Systems.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m unwatched_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(user_ratings[target_user] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Predict the ratings for the unwatched movies\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predicted_ratings \u001b[39m=\u001b[39m [predict_rating(user_ratings[target_user], user_ratings[:, movie_index]) \u001b[39mfor\u001b[39;00m movie_index \u001b[39min\u001b[39;00m unwatched_indices]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Sort the movies by the predicted rating in descending order\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m sorted_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(predicted_ratings)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/Users/yuqing/Desktop/MiningBigData/workshop mbd/workshop5/Workshop week 10 Association Mining and Recommender Systems.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ratings \u001b[39m=\u001b[39m movie_ratings[rated_indices]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Get the user vectors of the rated users\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m rated_users \u001b[39m=\u001b[39m user_ratings[rated_indices]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Calculate the similarities between the rated users and the target user\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuqing/Desktop/MiningBigData/workshop%20mbd/workshop5/Workshop%20week%2010%20Association%20Mining%20and%20Recommender%20Systems.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m similarities \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([similarity(user_ratings, rated_user) \u001b[39mfor\u001b[39;00m rated_user \u001b[39min\u001b[39;00m rated_users])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "def recommend_movies(user_ratings, target_user):\n",
    "    # Get the number of users and movies\n",
    "    num_users, num_movies = user_ratings.shape\n",
    "    \n",
    "    # Find the indices of the unwatched movies by the target user (where the rating == 0)\n",
    "    unwatched_indices = np.where(user_ratings[target_user] == 0)[0]\n",
    "    \n",
    "    # Predict the ratings for the unwatched movies\n",
    "    predicted_ratings = [predict_rating(user_ratings, user_ratings[:, movie_index]) for movie_index in unwatched_indices]\n",
    "    \n",
    "    # Sort the movies by the predicted rating in descending order\n",
    "    <your code>\n",
    "    \n",
    "    # Get the top 3 recommended movies\n",
    "    <your code>\n",
    "    \n",
    "    return recommended_movies\n",
    "\n",
    "# Create a sample ratings matrix\n",
    "ratings = np.array([[3, 0, 0, 5], [0, 4, 0, 3], [1, 0, 2, 4], [5, 0, 3, 0], [0, 2, 4, 0]])\n",
    "\n",
    "# Make movie recommendations for the target user\n",
    "# Choose a target user to make recommendations for\n",
    "target_user = 3\n",
    "recommended_movies = recommend_movies(ratings, target_user)\n",
    "\n",
    "# Print the recommended movies\n",
    "print(\"Recommended movies:\", recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# use pandas pivot to convert from\n",
    "#   userid, movieid, rating\n",
    "# to rating table with ratings for users in rows, where each movie rating is in column\n",
    "### the head of pandas frame looks like this:\n",
    "# <bound method NDFrame.head of userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
    "# movieId                                                    ...                  \n",
    "# 1        4.0  0.0  0.0  0.0  4.0  0.0  4.5  0.0  0.0  0.0  ...  4.0  0.0  4.0   \n",
    "# 2        0.0  0.0  0.0  0.0  0.0  4.0  0.0  4.0  0.0  0.0  ...  0.0  4.0  0.0   \n",
    "# 3        4.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
    "# 4        0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
    "# 5        0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
    "# ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
    "\n",
    "<Your code>\n",
    "\n",
    "'''\n",
    "In a real world, ratings are very sparse and data points are mostly collected \n",
    "from very popular movies and highly engaged users. \n",
    "So we will reduce the noise by adding some filters \n",
    "and qualify the movies for the final dataset.\n",
    "\n",
    "To qualify a movie, minimum 10 users should have voted a movie.\n",
    "To qualify a user, minimum 50 movies should have voted by the user.\n",
    "\n",
    "Implement a filter as specified above\n",
    "'''\n",
    "<your code>\n",
    "\n",
    "# finally,convert ratring into numpy to get\n",
    "#               mvi1, mvi2, ...\n",
    "#        user1  [4. ,   0. , 0. , ..., 4. , 2.5, 5. ],\n",
    "#        user2  [0. ,   0. , 4. , ..., 0. , 2. , 0. ],\n",
    "#        ...,\n",
    "\n",
    "rating  = <your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can test it for different target user ids\n",
    "target_user = 200\n",
    "recommended_movies = recommend_movies(ratings, target_user)\n",
    "\n",
    "# Print the recommended movies\n",
    "print(\"Recommended movies:\", recommended_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4: Tiktok Case Study\n",
    "\n",
    "Read the following article, discussing with your classmates, and respond to the following questions:\n",
    "\n",
    "Article: https://www.popsci.com/technology/tiktok-algorithm/ Why TikTok's algorithm is so addictive? \n",
    "\n",
    "The idea behind these questions is to prompt you to think more holistically about the industry on which you are studying. \n",
    "\n",
    "Technical skills can be taught, but it is crucial to consider the impact of the work you are undertaking, including the threats compared to the benefits.\n",
    "\n",
    "Questions: \n",
    "\n",
    "1) How does TikTok's recommendation algorithm leverage user interactions, such as likes, comments, watch time, and shares, to personalize the content feed?\n",
    "\n",
    "2) Do you think the level of personalization described in the article enhances or limits the user experience on TikTok?\n",
    "\n",
    "3) How do TikTok's human content moderators complement the work of the algorithm?\n",
    "\n",
    "4) What challenges and benefits might arise from the collaboration between automated algorithms and human moderation in content platforms?\n",
    "\n",
    "5) The article suggests that rapid growth can pose challenges for platforms like TikTok. In what ways might fast-paced growth impact a platform's ability to address and mitigate potential harms?\n",
    "\n",
    "6) The article mentions concerns about TikTok users encountering harmful content as their streams become more niche. What measures could platforms take to address this issue and protect users, especially considering TikTok's younger user base?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
